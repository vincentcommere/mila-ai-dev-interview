# ğŸ“˜ **RAPPORT TECHNIQUE â€” NVIDIA EARNINGS CALL RAG APP**

## 1. ğŸ§  **ComprÃ©hension du ProblÃ¨me**

Le dÃ©fi consiste Ã  construire une application capable de :

* **creer un chatbot ingÃ©rer avec des transcripts d'Earnings Calls NVIDIA**
* **filtrer, chunker, vectoriser et stocker ces informations dans une base vectorielle**
* **fournir une interface permettant de poser des questions**
* **retrouver rapidement des passages pertinents via un retrieve-and-generate (RAG)**
* **servir des rÃ©ponses fiables et contextualisÃ©es**
* **fonctionner dans une architecture DockerisÃ©e complÃ¨te**
* **Ãªtre simple Ã  installer et tester localement**

L'utilisateur final doit pouvoir **poser une question en langage naturel** et recevoir une **rÃ©ponse synthÃ©tique + les sources** issues des transcripts.

Lâ€™objectif gÃ©nÃ©ral :

> **Construire un systÃ¨me de Q&A performant, modulaire, reproductible et scalable en peu de temps.**

---

## 2. ğŸ—ï¸ **Design et Choix Techniques**

### 2.1. Choix du modÃ¨le dâ€™embedding

J'ai choisi :

> **BAAI/bge-base-en-v1.5**
> via SentenceTransformers (ou une alternative ONNX plus rapide) a des fin de demonstration de competences (du local load) dans le cadre du test. 
> utilisation via Hugging face api plus simple (sur le model du call LLM)

ğŸ¯ Pourquoi ce modÃ¨le ?

* petite taille
* excellent score sur MTEB (benchmark SOTA)
* trÃ¨s adaptÃ© aux tasks de **retrieval et semantic search**
* embeddings trÃ¨s cohÃ©rents pour des documents business / earnings calls
* stable et mature

*(Si version ONNX : modÃ¨le plus rapide, pas besoin de PyTorch â†’ idÃ©al Docker slim.)*
*(Si plus gros modÃ¨le, passer via API call plutÃ´t que local load)
---

### 2.2. Choix du Vector Store : **ChromaDB**

> **Chroma** est lÃ©ger, rapide, open-source, easy-to-use, parfait pour un prototype et suffisamment robuste pour de la prod.

Pourquoi Chroma ?

* facile Ã  containeriser
* API HTTP simple
* CRUD sur embeddings rapide
* support natif cosine similarity
* stockage persistant
* intÃ¨gre parfaitement avec Python

---

### 2.3. StratÃ©gie de chunking

* Chunk **500 tokens**, overlap 100

Pourquoi ce chunking ?

* chunks trop longs = bruit
* chunks trop courts = perte de contexte
* 500 tokens â†’ idÃ©al earnings calls (block logique = rÃ©ponse d'un speaker)

---

### 2.4. Choix de lâ€™architecture logicielle


#### **1. One-Time manual dâ€™ingestion (job one-shot)**

* charge les JSONL et chunks
* compute embeddings
* insÃ¨re dans Chroma

Architecture Microservices, composÃ©e de :

#### **2. Backend (FastAPI)**

* expose `/rag`
* rÃ©cupÃ¨re embeddings pertinents depuis Chroma - approche local loading
* prÃ©pare un prompt pour HuggingFace LLM - approche API Call
* gÃ©nÃ¨re la rÃ©ponse

#### **3. Frontend (React/Vite)**

* interface minimaliste pour poser une question
* affiche rÃ©ponse dans un chat

#### **4. Frontend (Nginx)**

* reverse proxy
* gÃ¨re le routing
* sert le frontend
* protÃ¨ge le backend

#### **5. Vector DB â€” Chroma (container)**

* indÃ©pendant
* persistant
* Ã©vite d'exploser le backend si Chroma reload
* scalable horizontalement
* Single responsability

---

## 3. ğŸ“Š **Data & Preprocessing**

* Earnings Calls rÃ©cupÃ©rÃ©s en fichiers `.jsonl`
* Nettoyage :
  * restructuration / epuration
  * dÃ©coupe en blocs par speaker
* Tokenisation + chunking
* GÃ©nÃ©ration dâ€™un **fichier `data/nvidia_chunks.jsonl`** pour ingestion dans chroma

---

## 4. ğŸ§¹ **Vectorisation** - choix du local loading

  NB :  choix du local loading volontaire pour la demo
* modÃ¨le : `BAAI/bge-base-en-v1.5`
* embeddings normalisÃ©s (`L2 norm`)
* stockage dans Chroma via lâ€™API HTTP
* structure dâ€™index : metric = cosine

---

## 5. ğŸ—„ï¸ **Vector Store : Chroma**

Chroma stocke :

* `ids`
* `transcript`
* `metadatas`
* `embeddings`

Pourquoi un container sÃ©parÃ© ?

* isolation mÃ©moire
* stabilitÃ©
* Ã©vite de polluer le backend
* reboot sans perte de donnÃ©es
* respect du principe "1 service = 1 responsabilitÃ©"

---

## 6. ğŸ—ï¸ **Architecture de la solution**

### ğŸ“Œ Vue dâ€™ensemble (diagramme ASCII)

```
                       +----------------------+
                       |      Frontend        |
                       |       React          |
                       +----------+-----------+
                                  |
                                  v
                         +--------+--------+
                         |     NGINX       |
                         |   Reverse Proxy |
                         +--------+--------+
                                  |
                                  v
                         +--------+--------+
                         |     FastAPI     |
                         |   Backend API   |
                         +--------+--------+
             retrieve â†’           |
                                  v
                        +---------+---------+
                        |     Chroma DB     |
                        |   Vector Store    |
                        +-------------------+

             one-shot ingestion job:
                        +-------------------+
                        |   ingest (job)    |
                        | setup_db.py       |
                        +-------------------+
```

---

## 7. âš™ï¸ **Backend Description**

### Endpoints

#### `POST`

- `/`  
  Health check â€“ retourne : `{"answer": "test ok !"}`

- `/dummy`  
  Retourne tel quel lâ€™input utilisateur (pour valider la communication frontend â†” backend)

- `/llm`  
  Appelle le LLM **sans** RAG et retourne une rÃ©ponse gÃ©nÃ©rÃ©e.

- `/rag`  
  ImplÃ©mente le RAG tel que spÃ©cifiÃ© (retriever + contexte + appel LLM).

---

## 8. ğŸ¨ **Frontend Description**

* UI simple en React
* trois composents principaux ChatBubble.jsx, InputBox.jsx, Messages.jsx
* responsive, extensilble au texte de differente taille


---

## 9. ğŸ§­ **Pourquoi NGINX ?**

* gÃ¨re le **routing** (frontend â†” backend)
* sert le build React en mode performant
* produit une architecture plus rÃ©aliste
* uniformise la app entry point 
* localhost reverse proxy
* permet SSL plus tard

---

## 10. ğŸ§± **Pourquoi un container dÃ©diÃ© pour Chroma ?**

* Ã©viter dâ€™installer tout Chroma dans le backend
* isolation mÃ©moire + CPU
* possibilitÃ© de scaling indÃ©pendant
* permet ingestion job sÃ©parÃ©
* pratique pour la persistance via volumes
* separation of concernes

---

## 11. ğŸ§ª **Setup & Run Instructions**

### 1. Clone

```
git clone https://github.com/vincentcommere/mila-ai-dev-interview.git
cd mila-ai-dev-interview 
```

### 2. Ajouter `./backend/.env` votre hugginface API_KEY pour inference endpoints

[https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)


```
API_KEY=xxxxx
```

### 3. Build/Run la vector DB 

```
make chroma-nocache
```

### 4. executer le one-time ingest  (Mac only)

```
make db_setup
```

### 5. Build/Run backend

```
make backend-nocache
```

### 6. Build/Run frontend

```
make frontend-nocache
```

### 7. Ouvrir le frontend

[http://localhost:80](http://localhost:80)


### 8. AprÃ¨s la premiÃ¨re requÃªte, prÃ©voir un dÃ©lai de quelques minutes pour permettre lâ€™initialisation complÃ¨te du retriever (chargement de la collection et du modÃ¨le dâ€™embeddings).
```
backend   | ğŸ”Œ Initializing Retriever...
backend   | ğŸ“š Retriever loaded collection: nvidia_earnings_calls
```

<p align="center">
  <img src="img/Screenshot 2025-11-20-4.png" width="500"/>
</p>


### 9. arreter tout

```
make down
```

---

## 12. ğŸ’¬ **Exemples de questions**

```
â— â€œWhat did Nvidia report about revenue last quarter?â€
â— â€œSummarize Nvidiaâ€™s Q2 2024 guidance.â€
â— â€œList key risks mentioned by Nvidia in Q4 FY23.â€
```
<p align="center">
  <img src="img/Screenshot 2025-11-19-1.png" width="450"/>
</p>
<p align="center">
  <img src="img/Screenshot 2025-11-19-2.png" width="450"/>
</p>
<p align="center">
  <img src="img/Screenshot 2025-11-19-3.png" width="450"/>
</p>
---

## 13. âš–ï¸ **Trade-offs**

* Jâ€™ai choisi Chroma plutÃ´t pour simplifier le Docker networking.
* Chunking simple 500 tokens : ok pour un prototype, mais amÃ©liorable.
* Pas dâ€™auth backend â€” trop long pour un proof-of-concept.
* Pas de citation exacte des paragraphes (option possible).


---

## 14. ğŸ¤– **Usage de GenAI dans le dÃ©veloppement**

* Jâ€™utilise ChatGPT comme un accÃ©lÃ©rateur dans mon processus de dÃ©veloppement, tout en conservant une maÃ®trise complÃ¨te du code produit.
* Je ne gÃ©nÃ¨re jamais de code que je ne comprends pas ou que je ne suis pas capable dâ€™adapter moi-mÃªme.

* Je nâ€™utilise pas dâ€™outils de gÃ©nÃ©ration automatique tels que Cursor ou GitHub Copilot.
* Lorsque je demande des suggestions dâ€™amÃ©lioration, je les Ã©value systÃ©matiquement et je dÃ©cide moi-mÃªme de leur pertinence avant de les intÃ©grer.
---

## 15. ğŸš€ **Suggestions de futurs travaux**


ğŸ”§ AmÃ©lioration des performances LLM

* Optimisation du modÃ¨le dâ€™embeddings et passage de celui-ci en API call
* Exploration et amÃ©lioration des mÃ©thodes de recherche RAG
* Ajustement des stratÃ©gies de dÃ©coupage (chunking)
* IntÃ©gration dâ€™un modÃ¨le de reranking (ex. bge-reranker)
* Utilisation dâ€™un modÃ¨le dâ€™embedding plus lÃ©ger et plus rapide

ğŸ—ï¸ Optimisation de lâ€™architecture

* RÃ©duction de la taille des images Docker (par ex. utilisation dâ€™images Alpine)
* Optimisation de certaines librairies lourdes (ex. Sentence Transformers avec Torch)
* Mise en place ou optimisation du Vector DB
* Optimisation de lâ€™initialisation du retriever au dÃ©marrage (actuellement plusieurs minutes), afin dâ€™Ã©viter les erreurs 504 Gateway Timeout lors des premiÃ¨res requÃªtes

ğŸ§‘â€ğŸ’» AmÃ©liorations logicielles

* AmÃ©lioration du frontend
* ImplÃ©mentation ou optimisation de lâ€™authentification
* Ajout de tests unitaires
* Ajout de tests dâ€™intÃ©gration
* Ajout ou amÃ©lioration du linting (Black, Flake8) et du typing
---
