# ğŸ“˜ **RAPPORT TECHNIQUE â€” NVIDIA EARNINGS CALL RAG APP**

## 1. ğŸ§  **ComprÃ©hension du ProblÃ¨me**

Le dÃ©fi consiste Ã  construire une application capable de :

* **ingÃ©rer des transcripts d'Earnings Calls NVIDIA**
* **chunker, vectoriser et stocker ces informations dans une base vectorielle**
* **fournir une interface permettant de poser des questions**
* **retrouver rapidement des passages pertinents via un retrieve-and-generate (RAG)**
* **servir des rÃ©ponses fiables et contextualisÃ©es**
* **fonctionner dans une architecture DockerisÃ©e complÃ¨te**
* **Ãªtre simple Ã  installer et tester localement**

L'utilisateur final doit pouvoir **poser une question en langage naturel** et recevoir une **rÃ©ponse synthÃ©tique + les sources** issues des transcripts.

Lâ€™objectif gÃ©nÃ©ral :

> **Construire un systÃ¨me de Q&A performant, modulaire, reproductible et scalable en peu de temps.**

---

## 2. ğŸ—ï¸ **Design et Choix Techniques**

### 2.1. Choix du modÃ¨le dâ€™embedding

J'ai choisi :

> **BAAI/bge-large-en-v1.5**
> via SentenceTransformers (ou une alternative ONNX plus rapide).

ğŸ¯ Pourquoi ce modÃ¨le ?

* excellent score sur MTEB (benchmark SOTA)
* trÃ¨s adaptÃ© aux tasks de **retrieval et semantic search**
* embeddings trÃ¨s cohÃ©rents pour des documents business / earnings calls
* stable et mature

*(Si version ONNX : modÃ¨le plus rapide, pas besoin de PyTorch â†’ idÃ©al Docker slim.)*

---

### 2.2. Choix du Vector Store : **ChromaDB**

> **Chroma** est lÃ©ger, rapide, open-source, easy-to-use, parfait pour un prototype et suffisamment robuste pour de la prod.

Pourquoi Chroma ?

* API HTTP simple
* CRUD sur embeddings rapide
* support natif cosine similarity
* stockage persistant
* facile Ã  containeriser
* intÃ¨gre parfaitement avec Python

---

### 2.3. StratÃ©gie de chunking

* Chunk **500 tokens**, overlap 100
* Format JSONL avec :

  ```json
  { "id": "...", "text": "...", "metadata": { "year": "...", "quarter": "..." } }
  ```

Pourquoi ce chunking ?

* chunks trop longs = bruit
* chunks trop courts = perte de contexte
* 500 tokens â†’ idÃ©al earnings calls (block logique = rÃ©ponse d'un speaker)

---

### 2.4. Choix de lâ€™architecture logicielle


#### **1. One-Time manual dâ€™ingestion (job one-shot)**

* charge les JSONL et chunks
* compute embeddings
* insÃ¨re dans Chroma

Architecture Microservices, composÃ©e de :

#### **2. Backend (FastAPI)**

* expose `/query`
* rÃ©cupÃ¨re embeddings pertinents depuis Chroma
* prÃ©pare un prompt pour HuggingFace LLM
* gÃ©nÃ¨re la rÃ©ponse

#### **3. Frontend (React/Vite)**

* interface minimaliste pour poser une question
* affiche rÃ©ponse + sources

#### **4. Reverse proxy â€” Nginx**

* gÃ¨re le routing
* sert le frontend
* protÃ¨ge le backend
* force CORS & SSL si besoin

#### **5. Vector DB â€” Chroma container**

* indÃ©pendant
* persistant
* Ã©vite d'exploser le backend si Chroma reload
* scalable horizontalement

---

## 3. ğŸ“Š **Data & Preprocessing**

* Earnings Calls rÃ©cupÃ©rÃ©s en fichiers `.txt`
* Nettoyage :

  * suppression timestamps
  * normalisation whitespaces
  * dÃ©coupe en blocs par speaker
* Tokenisation + chunking
* GÃ©nÃ©ration dâ€™un **fichier `data/nvidia_chunks.jsonl`**

---

## 4. ğŸ§¹ **Vectorisation**

* modÃ¨le : `BAAI/bge-large-en-v1.5`
* embeddings normalisÃ©s (`L2 norm`)
* stockage dans Chroma via lâ€™API HTTP
* structure dâ€™index : HNSW, metric = cosine

---

## 5. ğŸ—„ï¸ **Vector Store : Chroma**

Chroma stocke :

* `ids`
* `documents`
* `metadatas`
* `embeddings`

AccÃ¨s rapide (O(log n)) via index HNSW.

Pourquoi un container sÃ©parÃ© ?

* isolation mÃ©moire
* stabilitÃ©
* Ã©vite de polluer le backend
* reboot sans perte de donnÃ©es
* respect du principe "1 service = 1 responsabilitÃ©"

---

## 6. ğŸ—ï¸ **Architecture de la solution**

### ğŸ“Œ Vue dâ€™ensemble (diagramme ASCII)

```
                       +----------------------+
                       |      Frontend        |
                       |       React          |
                       +----------+-----------+
                                  |
                                  v
                         +--------+--------+
                         |     NGINX       |
                         |   Reverse Proxy |
                         +--------+--------+
                                  |
                                  v
                         +--------+--------+
                         |     FastAPI     |
                         |   Backend API   |
                         +--------+--------+
             retrieve â†’           |
                                  v
                        +---------+---------+
                        |     Chroma DB     |
                        |   Vector Store    |
                        +-------------------+

             one-shot ingestion job:
                        +-------------------+
                        |   ingest (job)    |
                        | setup_db.py       |
                        +-------------------+
```

---

## 7. âš™ï¸ **Backend Description**

### Endpoints

#### `POST /query`

/ # health check return {"answer":"test ok !"}
/dummy # return user input to validation frontend backend comunication
/llm # answer using llm without rag
/rag # implement le rag tel que demande

---

## 8. ğŸ¨ **Frontend Description**

* UI simple en React
* trois composents principaux ChatBubble.jsx, InputBox.jsx, Messages.jsx
* responsive, extensilble au texte de differente taille


---

## 9. ğŸ§­ **Pourquoi NGINX ?**

* gÃ¨re le **routing** (frontend â†” backend)
* sert le build React en mode performant
* produit une architecture plus rÃ©aliste
* uniformise la app entry point 
* localhost reverse proxy
* permet SSL plus tard

---

## 10. ğŸ§± **Pourquoi un container dÃ©diÃ© pour Chroma ?**

* Ã©viter dâ€™installer tout Chroma dans le backend
* isolation mÃ©moire + CPU
* possibilitÃ© de scaling indÃ©pendant
* permet ingestion job sÃ©parÃ©
* pratique pour la persistance via volumes
* separation of concernes

---

## 11. ğŸ§ª **Setup & Run Instructions**

### 1. Clone

```
git clone https://github.com/vincentcommere/mila-ai-dev-interview.git
cd mila-ai-dev-interview 
```

### 2. Ajouter `./backend/.env` votre hugginface API_KEY pour inference endpoints

```
API_KEY=xxxxx
```

### 3. Build/Run la vector DB 

```
make chroma-nocache
```

### 4. executer le one-time ingest  (Mac only)

```
cd ingest
python3 -m venv venv
. venv/bin/activate
pip install -r requirements.txt
python setup_db.py 
deactivate
cd ..
```

### 5. Build/Run backend

```
make backend-nocache
```

### 6. Build/Run frontend

```
make frontend-nocache
```

### 7. Ouvrir le frontend

[http://localhost:80](http://localhost:80)


### 8. Patienter quelque minute a lissue de la premiere requete afin que le retriever sinisalise ( load collection, load embeddings models)

```
backend   | ğŸ”Œ Initializing Retriever...
backend   | ğŸ“š Retriever loaded collection: nvidia_earnings_calls
```

### 9. arreter tout

```
make down
```

---

## 12. ğŸ’¬ **Exemples de questions**

```
â— â€œWhat did Nvidia report about revenue last quarter?â€
â— â€œSummarize Nvidiaâ€™s Q2 2024 guidance.â€
â— â€œList key risks mentioned by Nvidia in Q4 FY23.â€
```
<p align="center">
  <img src="img/Screenshot 2025-11-19-1.png" width="450"/>
</p>
<p align="center">
  <img src="img/Screenshot 2025-11-19-2.png" width="450"/>
</p>
<p align="center">
  <img src="img/Screenshot 2025-11-19-3.png" width="450"/>
</p>
---

## 13. âš–ï¸ **Trade-offs**

* Jâ€™ai choisi Chroma plutÃ´t pour simplifier le Docker networking.
* Chunking simple 500 tokens : ok pour un prototype, mais amÃ©liorable.
* Pas dâ€™auth backend â€” trop long pour un proof-of-concept.
* Pas de citation exacte des paragraphes (option possible).

---

## 14. ğŸ¤– **Usage de GenAI dans le dÃ©veloppement**

* jai utilise chat gpt, je ne genere pas de code que je ne comprend ou ne metreise pas jutilise lia pour accelerer ce que je veux faire. je nai pas utilise cursor ou co pilot, je demande egalement quels sont els amelioration que je peut faire, puis jaccepte ou nom cell ci 

---

## 15. ğŸš€ **Suggestions de futurs travaux**


Perfomance LLM
  - embeddings model
  - RAG search methodes
  - chunks methodes
  - intÃ©grer reranking **bge-reranker**
  - model embedding plus leger

Perfomance Architecture
- reduiction du build taille des images peuvent etre reduit a parti dimage alpine
- reduiction du build les utiliseation de certainses librairies peuvent etre optimiset (Sentence Transformer qui utilise torch par exemple)
- Vector DB setup up
- At startup, retriever inittialisation peut etre optimise car il prend plusieurs minutes, les premieres requestes genere parfois des A 504 Gateway Timeout error 

Software
- Frontend
- authentification
- test unitaire
- test integration
- linting (blakc, flake8) et typing


---
