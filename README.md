# üìò **RAPPORT TECHNIQUE ‚Äî NVIDIA EARNINGS CALL RAG APP**

## 1. üß† **Compr√©hension du Probl√®me**

Le d√©fi consiste √† construire une application capable de :

* **ing√©rer des transcripts d'Earnings Calls NVIDIA**
* **chunker, vectoriser et stocker ces informations dans une base vectorielle**
* **fournir une interface permettant de poser des questions**
* **retrouver rapidement des passages pertinents via un retrieve-and-generate (RAG)**
* **servir des r√©ponses fiables et contextualis√©es**
* **fonctionner dans une architecture Dockeris√©e compl√®te**
* **√™tre simple √† installer et tester localement**

L'utilisateur final doit pouvoir **poser une question en langage naturel** et recevoir une **r√©ponse synth√©tique + les sources** issues des transcripts.

L‚Äôobjectif g√©n√©ral :

> **Construire un syst√®me de Q&A performant, modulaire, reproductible et scalable en peu de temps.**

---

## 2. üèóÔ∏è **Design et Choix Techniques**

### 2.1. Choix du mod√®le d‚Äôembedding

J'ai choisi :

> **BAAI/bge-large-en-v1.5**
> via SentenceTransformers (ou une alternative ONNX plus rapide).

üéØ Pourquoi ce mod√®le ?

* excellent score sur MTEB (benchmark SOTA)
* tr√®s adapt√© aux tasks de **retrieval et semantic search**
* embeddings tr√®s coh√©rents pour des documents business / earnings calls
* stable et mature

*(Si version ONNX : mod√®le plus rapide, pas besoin de PyTorch ‚Üí id√©al Docker slim.)*

---

### 2.2. Choix du Vector Store : **ChromaDB**

> **Chroma** est l√©ger, rapide, open-source, easy-to-use, parfait pour un prototype et suffisamment robuste pour de la prod.

Pourquoi Chroma ?

* API HTTP simple
* CRUD sur embeddings rapide
* support natif cosine similarity
* stockage persistant
* facile √† containeriser
* int√®gre parfaitement avec Python

---

### 2.3. Strat√©gie de chunking

* Chunk **500 tokens**, overlap 100
* Format JSONL avec :

  ```json
  { "id": "...", "text": "...", "metadata": { "year": "...", "quarter": "..." } }
  ```

Pourquoi ce chunking ?

* chunks trop longs = bruit
* chunks trop courts = perte de contexte
* 500 tokens ‚Üí id√©al earnings calls (block logique = r√©ponse d'un speaker)

---

### 2.4. Choix de l‚Äôarchitecture logicielle

Architecture Microservices, compos√©e de :

#### **1. Service d‚Äôingestion (job one-shot)**

* Dockerfile d√©di√©
* charge les JSONL
* compute embeddings
* ins√®re dans Chroma

#### **2. Backend (FastAPI)**

* expose `/query`
* r√©cup√®re embeddings pertinents depuis Chroma
* pr√©pare un prompt pour HuggingFace LLM
* g√©n√®re la r√©ponse

#### **3. Frontend (React/Vite)**

* interface minimaliste pour poser une question
* affiche r√©ponse + sources

#### **4. Reverse proxy ‚Äî Nginx**

* g√®re le routing
* sert le frontend
* prot√®ge le backend
* force CORS & SSL si besoin

#### **5. Vector DB ‚Äî Chroma container**

* ind√©pendant
* persistant
* √©vite d'exploser le backend si Chroma reload
* scalable horizontalement

---

## 3. üìä **Data & Preprocessing**

* Earnings Calls r√©cup√©r√©s en fichiers `.txt`
* Nettoyage :

  * suppression timestamps
  * normalisation whitespaces
  * d√©coupe en blocs par speaker
* Tokenisation + chunking
* G√©n√©ration d‚Äôun **fichier `data/nvidia_chunks.jsonl`**

---

## 4. üßπ **Vectorisation**

* mod√®le : `BAAI/bge-large-en-v1.5`
* embeddings normalis√©s (`L2 norm`)
* stockage dans Chroma via l‚ÄôAPI HTTP
* structure d‚Äôindex : HNSW, metric = cosine

---

## 5. üóÑÔ∏è **Vector Store : Chroma**

Chroma stocke :

* `ids`
* `documents`
* `metadatas`
* `embeddings`

Acc√®s rapide (O(log n)) via index HNSW.

Pourquoi un container s√©par√© ?

* isolation m√©moire
* stabilit√©
* √©vite de polluer le backend
* reboot sans perte de donn√©es
* respect du principe "1 service = 1 responsabilit√©"

---

## 6. üèóÔ∏è **Architecture de la solution**

### üìå Vue d‚Äôensemble (diagramme ASCII)

```
                       +----------------------+
                       |      Frontend        |
                       |       React          |
                       +----------+-----------+
                                  |
                                  v
                         +--------+--------+
                         |     NGINX       |
                         |   Reverse Proxy |
                         +--------+--------+
                                  |
                                  v
                         +--------+--------+
                         |     FastAPI     |
                         |   Backend API   |
                         +--------+--------+
             retrieve ‚Üí           |
                                  v
                        +---------+---------+
                        |     Chroma DB     |
                        |   Vector Store    |
                        +-------------------+

             one-shot ingestion job:
                        +-------------------+
                        |   ingest (job)    |
                        | setup_db.py       |
                        +-------------------+
```

---

## 7. ‚öôÔ∏è **Backend Description**

### Endpoints

#### `POST /query`

Input :

```json
{ "question": "What did Jensen say about gaming revenue in Q4 2023?" }
```

Process :

1. embedded la question
2. cherche les vecteurs les plus proches dans Chroma
3. construit un prompt
4. envoie au mod√®le HF
5. retourne une r√©ponse + chunks sources

Output :

```json
{
  "answer": "...",
  "sources": [
    { "id": "...", "text": "..." }
  ]
}
```

---

## 8. üé® **Frontend Description**

* UI simple en React
* champ de texte pour poser une question
* affichage des r√©sultats avec highlight
* affichage des sources

Pourquoi React ?

* rapide √† mettre en place
* facile √† dockeriser avec Nginx
* moderne et maintenable

---

## 9. üß≠ **Pourquoi NGINX ?**

* g√®re le **routing** (frontend ‚Üî backend)
* sert le build React en mode performant
* produit une architecture plus r√©aliste
* ajoute CORS, headers de s√©curit√©
* permet SSL plus tard

---

## 10. üß± **Pourquoi un container d√©di√© pour Chroma ?**

* √©viter d‚Äôinstaller tout Chroma dans le backend
* isolation m√©moire + CPU
* possibilit√© de scaling ind√©pendant
* permet ingestion job s√©par√©
* pratique pour la persistance via volumes

---

## 11. üß™ **Setup & Run Instructions**

### 1. Clone

```
git clone <repo-url>
cd project
```

### 2. Ajouter `.env`

```
HF_API_KEY=xxxxx
```

### 3. Lancer l‚Äôapplication

```
docker compose up -d --build
```

### 4. V√©rifier l‚Äôingestion

```
docker logs ingest -f
```

Si succ√®s :

```
üî• Successfully inserted XXXX vectors
```

### 5. Ouvrir le frontend

[http://localhost:80](http://localhost:80)

---

## 12. üí¨ **Exemples de questions**

```
"What did Jensen say about Data Center business growth?"
"How did Gaming revenue evolve in Q2 2023?"
"What guidance was provided for next quarter?"
```

---

## 13. ‚öñÔ∏è **Trade-offs**

* J‚Äôai choisi FastAPI plut√¥t que LangChain pour plus de contr√¥le.
* J‚Äôai choisi Chroma plut√¥t que FAISS pour simplifier le Docker networking.
* Chunking simple 500 tokens : ok pour un prototype, mais am√©liorable.
* Pas d‚Äôauth backend ‚Äî trop long pour un proof-of-concept.
* Pas de citation exacte des paragraphes (option possible).

---

## 14. ü§ñ **Usage de GenAI dans le d√©veloppement**

* g√©n√©ration initiale des mod√®les d‚Äôarchitectures
* tests de chunking et pipeline embedding
* g√©n√©ration partielle de code boilerplate
* optimisation ultrarapide du Dockerfile et services
* documentation + rapport g√©n√©r√© en LLM

---

## 15. üöÄ **Suggestions de futurs travaux**

* utiliser un mod√®le ONNX pour r√©duire l‚Äôimage
* int√©grer reranking **bge-reranker**
* ajouter summarization des earnings calls
* am√©liorer le frontend (citations, highlights)
* support multi-compagnies / multi-documents
* auth Oauth2 + logs d‚Äôusage
* CI/CD GitHub actions + tests unittaires

---

# üéâ Rapport termin√©

Si tu veux :

* une **version PDF**
* une **version Markdown GitHub**
* un **diagramme mermaid**
* une **pr√©sentation PowerPoint** g√©n√©r√©e
  ‚Üí Dis-moi, je te la g√©n√®re.
